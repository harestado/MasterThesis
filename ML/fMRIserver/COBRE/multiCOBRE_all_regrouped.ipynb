{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, recall_score, precision_score, classification_report\n",
    "\n",
    "from train_model_multi_grouped_bal import train_model\n",
    "from validate_multi import validate\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "BATCH_SIZE = 1 #5\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 0.01\n",
    "DROPOUT_RATE = 0.2\n",
    "LAYER_SIZE = 5\n",
    "SPLIT = 0.45\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COBRE_dataset_subj(torch.utils.data.Dataset):\n",
    "    \"\"\"This Dataset class loads pairs of COBRE images and labels into the computer memory. The entire subject file is loaded.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path (str): Path to the folder containing images and labels\n",
    "    Returns:\n",
    "    - Tensor: Torch tensor with COBRE time-series\n",
    "    - int: label (0 or 1)\n",
    "\n",
    "   \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.num_subjects = len(os.listdir(self.data_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_subjects\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        subject = os.listdir(self.data_path)[index]\n",
    "        img = self.data_path + '/' + subject + '/session_1/rest_1/nourest.nii'\n",
    "        label = self.data_path + '/' + subject + '/session_1/' + subject + '_data.csv'\n",
    "\n",
    "        img = nib.load(img).get_fdata()\n",
    "        img = np.swapaxes(img, 0, 3)\n",
    "        img = torch.from_numpy(img)\n",
    "        img = img.to(torch.float)\n",
    "\n",
    "        label = pd.read_csv(label)\n",
    "        subject_type = label.iloc[0]['Subject Type']\n",
    "\n",
    "        ### LABEL\n",
    "        if subject_type == 'Patient':\n",
    "            label = str(label.iloc[0]['Diagnosis'])\n",
    "            if label == '295.1' or label ==  '295.2' or label ==  '295.3' or label ==  '295.6' or label ==  '295.9':\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 2\n",
    "        elif subject_type == 'Control':\n",
    "            label = 0\n",
    "        else:\n",
    "            print('Something wrong with data label:', end=' ')\n",
    "            print(label)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control\n",
      "40 35 4\n",
      "\n",
      "Val\n",
      "18 15 2\n",
      "\n",
      "Test\n",
      "14 12 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "control_path = 'C:/Users/oscar/OneDrive - University of Bergen/Documents/Master/vsc/COBRE_learning/data/regrouped/control'\n",
    "paranoid_path = 'C:/Users/oscar/OneDrive - University of Bergen/Documents/Master/vsc/COBRE_learning/data/regrouped/paranoid'\n",
    "rest_path = 'C:/Users/oscar/OneDrive - University of Bergen/Documents/Master/vsc/COBRE_learning/data/regrouped/rest'\n",
    "\n",
    "control_data = COBRE_dataset_subj(control_path)\n",
    "paranoid_data = COBRE_dataset_subj(paranoid_path)\n",
    "rest_data = COBRE_dataset_subj(rest_path)\n",
    "\n",
    "seed = torch.manual_seed(1337)\n",
    "\n",
    "train_control, test_val_control = random_split(control_data, [1-SPLIT, SPLIT], seed)\n",
    "train_paranoid, test_val_paranoid = random_split(paranoid_data, [1-SPLIT, SPLIT], seed)\n",
    "train_rest, test_val_rest = random_split(rest_data, [1-SPLIT, SPLIT], seed)\n",
    "\n",
    "val_control, test_control = random_split(test_val_control, [0.55, 0.45], seed)\n",
    "val_paranoid, test_paranoid = random_split(test_val_paranoid, [0.55, 0.45], seed)\n",
    "val_rest, test_rest = random_split(test_val_rest, [0.55, 0.45], seed)\n",
    "\n",
    "print('Control')\n",
    "print(len(train_control), len(train_paranoid), len(train_rest))\n",
    "print()\n",
    "print('Val')\n",
    "print(len(val_control), len(val_paranoid), len(val_rest))\n",
    "print()\n",
    "print('Test')\n",
    "print(len(test_control), len(test_paranoid), len(test_rest))\n",
    "print()\n",
    "\n",
    "train_data = ConcatDataset([train_control, train_paranoid, train_rest])\n",
    "val_data = ConcatDataset([val_control, val_paranoid, val_rest])\n",
    "# test_data = ConcatDataset([test_control, test_paranoid, test_rest])\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, generator=seed, num_workers=0)\n",
    "valloader = DataLoader(val_data, batch_size=1, shuffle=True, generator=seed, num_workers=0)\n",
    "# testloader = DataLoader(test_data, batch_size=1, shuffle=True, generator=seed, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_Mari(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate, layer_size):\n",
    "        super(UNET_Mari, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = (nn.Conv3d(150, layer_size, kernel_size=(3, 3, 3), padding=1))\n",
    "        self.conv1_2 = (nn.Conv3d(layer_size, 1, kernel_size=(3, 3, 3), padding=1))\n",
    "        self.pool1 = (nn.MaxPool3d(kernel_size=(2,2,2), stride=2))\n",
    "        self.dropout = (nn.Dropout(dropout_rate))\n",
    "        self.conv2 = (nn.Conv2d(47, 1, kernel_size=(3, 3), padding=1))\n",
    "        # also more layers here, perhaps\n",
    "        self.pool2 = (nn.MaxPool2d(kernel_size=(2,2), stride=2))\n",
    "        self.fc1 = (nn.Linear(19*19, 4*4)) # one functional layer or two?\n",
    "        self.fc2 = (nn.Linear(4*4, num_classes))\n",
    "\n",
    "    def forward(self, x): # should ReLU be used in every step here?\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool1(x)\n",
    "        x = torch.squeeze(x, dim=1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "unet = UNET_Mari(NUM_CLASSES, DROPOUT_RATE, LAYER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, losses_val, losses_train, accuracies_val, accuracies_train, balanced_accuracies, balanced_accuracies_train, best_epoch = train_model(unet, device, trainloader, valloader, N_EPOCHS, LEARNING_RATE, DROPOUT_RATE, LAYER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = UNET_Mari(NUM_CLASSES, DROPOUT_RATE, LAYER_SIZE)\n",
    "best_model.load_state_dict(best_params)\n",
    "\n",
    "# best_model = torch.load('C:/Users/oscar/OneDrive - University of Bergen/Documents/Master/vsc/COBRE_learning/multilabel_regrouped/models/1503_0.01_0.2_5/best_model_057407_bal.pt')\n",
    "\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in best_model.parameters():\n",
    "#     if param.requires_grad:\n",
    "#         param = param\n",
    "#         break\n",
    "# param = deepcopy(param)\n",
    "# param = param.to('cpu').detach().numpy()\n",
    "# plt.imshow(param)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_scan = nib.load('C:/Users/oscar/OneDrive - University of Bergen/Documents/Master/vsc/my_scan/fMRI_231207_ERC2VF_100/005_fMRI_default-pulsm_ling/nou005_fMRI_default-pulsm_ling.nii')\n",
    "# my_scan = my_scan.get_fdata()\n",
    "# my_scan = np.swapaxes(my_scan, 0, 3)\n",
    "# my_scan = torch.from_numpy(my_scan)\n",
    "# my_scan = my_scan.to(torch.float)\n",
    "# my_scan = my_scan[0:150]\n",
    "# my_scan = my_scan.to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     pred = best_model(my_scan)\n",
    "#     _, predicted = torch.max(pred.data, 1)\n",
    "# print(pred)\n",
    "# print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Val')\n",
    "y_true, y_pred = validate(best_model, device, valloader)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train')\n",
    "y_true_train, y_pred_train = validate(best_model, device, trainloader)\n",
    "print(classification_report(y_true_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### METRICS\n",
    "# print('Val')\n",
    "# print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "# print('F1-score:', f1_score(y_true, y_pred, average='weighted'))\n",
    "# print('Balanced accuracy score:', balanced_accuracy_score(y_true, y_pred))\n",
    "# print('Balanced accuracy score:', balanced_accuracies[best_epoch-1])\n",
    "# print('Recall:', recall_score(y_true, y_pred, average='weighted'))\n",
    "# print('Precision:', precision_score(y_true, y_pred, average='weighted'))\n",
    "# print()\n",
    "# print('Train')\n",
    "# print('Accuracy:', accuracy_score(y_true_train, y_pred_train))\n",
    "# print('F1-score:', f1_score(y_true_train, y_pred_train, average='weighted'))\n",
    "# print('Balanced accuracy score:', balanced_accuracy_score(y_true_train, y_pred_train))\n",
    "# print('Balanced accuracy score:', balanced_accuracies_train[best_epoch-1])\n",
    "# print('Recall:', recall_score(y_true_train, y_pred_train, average='weighted'))\n",
    "# print('Precision:', precision_score(y_true_train, y_pred_train, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Val')\n",
    "print('True:', y_true)\n",
    "unique, count = np.unique(y_true, return_counts=True)\n",
    "print(unique, count)\n",
    "print('Pred:', y_pred)\n",
    "unique, count = np.unique(y_pred, return_counts=True)\n",
    "print(unique, count)\n",
    "unique, count = np.unique([t==y for t,y in zip(y_true, y_pred)], return_counts=True)\n",
    "print(unique, count)\n",
    "print('Number of wrong predictions:', [count[0] if len(count) != 1 else 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train')\n",
    "print('True:', y_true_train)\n",
    "unique, count = np.unique(y_true_train, return_counts=True)\n",
    "print(unique, count)\n",
    "print('Pred:', y_pred_train)\n",
    "unique, count = np.unique(y_pred_train, return_counts=True)\n",
    "print(unique, count)\n",
    "unique, count = np.unique([t==y for t,y in zip(y_true_train, y_pred_train)], return_counts=True)\n",
    "print(unique, count)\n",
    "print('Number of wrong predictions:', [count[0] if len(count) != 1 else 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### VAL MAP\n",
    "# map = [t==p for t,p in zip(y_true, y_pred)]\n",
    "# data_path = 'C:/Users/oscar/OneDrive - University of Bergen/Documents/Master/vsc/COBRE_learning/data/val'\n",
    "\n",
    "# for i in range(len(val)):\n",
    "#     if(not map[i]):\n",
    "#         _, label = val[i]\n",
    "#         # label = data_path + '/' + subject + '/' + subject + '_data.csv'\n",
    "#         # label = pd.read_csv(label)\n",
    "#         # label = label.iloc[0]\n",
    "#         print(label)\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TRAIN MAP\n",
    "# map = [t==p for t,p in zip(y_true_train, y_pred_train)]\n",
    "# data_path = 'C:/Users/oscar/OneDrive - University of Bergen/Documents/Master/vsc/COBRE_learning/data/train'\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     if(not map[i]):\n",
    "#         subject = os.listdir(data_path)[i]\n",
    "#         label = data_path + '/' + subject + '/' + subject + '_data.csv'\n",
    "#         label = pd.read_csv(label)\n",
    "#         label = label.iloc[0]\n",
    "#         print(label)\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime('%d%m')\n",
    "# date = str(1503) # March 15th\n",
    "data_dir = 'C:/Users/oscar/OneDrive - University of Bergen/Documents/Master/vsc/COBRE_learning/multilabel_regrouped/model_acc&loss/' + date + '_' + str(LEARNING_RATE) + '_' + str(DROPOUT_RATE) + '_' +  str(LAYER_SIZE)\n",
    "\n",
    "validation_losses = torch.load(data_dir + '/losses_val')\n",
    "train_losses = torch.load(data_dir + '/losses_train')\n",
    "validation_accuracies = torch.load(data_dir + '/acc_val')\n",
    "train_accuracies = torch.load(data_dir + '/acc_train')\n",
    "validation_bal_acc = torch.load(data_dir + '/bal_acc_val')\n",
    "train_bal_acc = torch.load(data_dir + '/bal_acc_train')\n",
    "best_epoch = np.argmax(validation_bal_acc) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvalues = np.linspace(1, N_EPOCHS, len(validation_losses))\n",
    "yvalues01 = validation_losses\n",
    "yvalues02 = train_losses\n",
    "name = 'Dropout=' + str(DROPOUT_RATE)\n",
    "\n",
    "plt.plot(xvalues, yvalues01, label=name + \" val loss\", color='r')\n",
    "plt.plot(xvalues, yvalues02, label=name + \" train loss\", color='b', linestyle=\"dashed\")\n",
    "max_value = 4.7 #np.max([np.max(validation_losses), np.max(train_losses)])\n",
    "min_value = np.min([np.min(validation_losses), np.min(train_losses)])\n",
    "plt.plot([best_epoch, best_epoch], [min_value,max_value], alpha=0.5, linewidth=3, label='Chosen model')\n",
    "\n",
    "plt.title('Validation and training loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xticks((np.array([0.1,1,2,3,4,5,6,7,8,9,10])/10)*N_EPOCHS)\n",
    "plt.ylim(0,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvalues = np.linspace(1, N_EPOCHS, len(validation_accuracies))\n",
    "yvalues01 = validation_accuracies\n",
    "yvalues02 = train_accuracies\n",
    "name = 'Dropout=' + str(DROPOUT_RATE)\n",
    "\n",
    "plt.plot(xvalues, yvalues01, label=name + \" val acc\", color='r')\n",
    "plt.plot(xvalues, yvalues02, label=name + \" train acc\", color='b', linestyle=\"dashed\")\n",
    "plt.plot([best_epoch, best_epoch], [0,1], alpha=0.5, linewidth=3, label='Chosen model')\n",
    "\n",
    "plt.title('Validation and training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xticks((np.array([1,2,3,4,5,6,7,8,9,10])/10)*N_EPOCHS)\n",
    "plt.yticks(np.array([0,1,2,3,4,5,6,7,8,9,10])/10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvalues = np.linspace(1, N_EPOCHS, len(validation_bal_acc))\n",
    "yvalues01 = validation_bal_acc\n",
    "yvalues02 = train_bal_acc\n",
    "name = 'Dropout=' + str(DROPOUT_RATE)\n",
    "\n",
    "plt.plot(xvalues, yvalues01, label=name + \" val bal acc\", color='r')\n",
    "plt.plot(xvalues, yvalues02, label=name + \" train bal acc\", color='b', linestyle=\"dashed\")\n",
    "plt.plot([best_epoch, best_epoch], [0,1], alpha=0.5, linewidth=3, label='Chosen model')\n",
    "\n",
    "plt.title('Validation and training balanced accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Balanced accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xticks((np.array([1,2,3,4,5,6,7,8,9,10])/10)*N_EPOCHS)\n",
    "plt.yticks(np.array([0,1,2,3,4,5,6,7,8,9,10])/10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
