{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, recall_score, precision_score, classification_report\n",
    "\n",
    "from train_model_multi_grouped_bal import train_model\n",
    "from validate_multi import validate\n",
    "\n",
    "# setting seeds for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "BATCH_SIZE = _\n",
    "N_EPOCHS = _\n",
    "LEARNING_RATE = _\n",
    "DROPOUT_RATE = _\n",
    "LAYER_SIZE = _\n",
    "SPLIT = 0.45\n",
    "MODE = 'bal' # 'acc'\n",
    "BINARY = True # False\n",
    "if BINARY:\n",
    "    NUM_CLASSES = 2\n",
    "else:\n",
    "    NUM_CLASSES = 3\n",
    "print(NUM_CLASSES)\n",
    "\n",
    "device = \"cpu\" #(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERC_I_dataset_subj(torch.utils.data.Dataset):\n",
    "    \"\"\"This Dataset class loads pairs of ERC-I images and labels.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path (str): Path to the folder containing images and labels\n",
    "    Returns:\n",
    "    - Tensor: Torch tensor with ERC_I time-series\n",
    "    - int: label (0, 1 or 2)\n",
    "\n",
    "   \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.num_subjects = len(os.listdir(self.data_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_subjects\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        subject = os.listdir(self.data_path)[index]\n",
    "        img = self.data_path + '/' + subject + '/functional.nii'\n",
    "        label = self.data_path + '/' + subject + '/pheno.csv'\n",
    "\n",
    "        img = nib.load(img).get_fdata()\n",
    "        img = np.swapaxes(img, 0, 3)\n",
    "        img = img[0:150]\n",
    "        if img.shape[0] != 150:\n",
    "            print(subject)\n",
    "        img = torch.from_numpy(img)\n",
    "        img = img.to(torch.float)\n",
    "\n",
    "        label = pd.read_csv(label)\n",
    "        diagnosis = label.iloc[0]['Diagnosis']\n",
    "\n",
    "        ### LABEL\n",
    "        if diagnosis == 'Ingen':\n",
    "            return img, 0\n",
    "        else:\n",
    "            if BINARY:\n",
    "                return img, 1\n",
    "            else:\n",
    "                diagnosis = label.iloc[0]['Group']\n",
    "                return img, diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERC_II_dataset_subj(torch.utils.data.Dataset):\n",
    "    \"\"\"This Dataset class loads pairs of ERC-II images and labels.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path (str): Path to the folder containing images and labels\n",
    "    Returns:\n",
    "    - Tensor: Torch tensor with ERC_II time-series\n",
    "    - int: label (0, 1 or 2)\n",
    "\n",
    "   \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.num_subjects = len(os.listdir(self.data_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_subjects\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        subject = os.listdir(self.data_path)[index]\n",
    "        img = self.data_path + '/' + subject + '/functional.nii'\n",
    "        label = self.data_path + '/' + subject + '/pheno.csv'\n",
    "\n",
    "        img = nib.load(img).get_fdata()\n",
    "        img = np.swapaxes(img, 0, 3)\n",
    "        img = img[0:150]\n",
    "        if img.shape[0] != 150:\n",
    "            print(subject)\n",
    "        img = torch.from_numpy(img)\n",
    "        img = img.to(torch.float)\n",
    "\n",
    "        label = pd.read_csv(label)\n",
    "        diagnosis = label.iloc[0]['Diagnosis']\n",
    "\n",
    "        ### LABEL\n",
    "        if diagnosis == 0:\n",
    "            return img, 0\n",
    "        else:\n",
    "            if BINARY:\n",
    "                return img, 1\n",
    "            else:\n",
    "                diagnosis = label.iloc[0]['Include']\n",
    "                return img, diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COBRE_dataset_subj(torch.utils.data.Dataset):\n",
    "    \"\"\"This Dataset class loads pairs of COBRE images and labels.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path (str): Path to the folder containing images and labels\n",
    "    Returns:\n",
    "    - Tensor: Torch tensor with COBRE time-series\n",
    "    - int: label (0, 1 or 2)\n",
    "\n",
    "   \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.num_subjects = len(os.listdir(self.data_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_subjects\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        subject = os.listdir(self.data_path)[index]\n",
    "        img = self.data_path + '/' + subject + '/functional.nii'\n",
    "        label = self.data_path + '/' + subject + '/pheno.csv'\n",
    "\n",
    "        img = nib.load(img).get_fdata()\n",
    "        img = np.swapaxes(img, 0, 3)\n",
    "        img = img[0:150]\n",
    "        if img.shape[0] != 150:\n",
    "            print(subject)\n",
    "        img = torch.from_numpy(img)\n",
    "        img = img.to(torch.float)\n",
    "\n",
    "        label = pd.read_csv(label)\n",
    "        diagnosis = label.iloc[0]['Subject Type']\n",
    "\n",
    "        ### LABEL\n",
    "        if diagnosis == 'Control':\n",
    "            return img, 0\n",
    "        else:\n",
    "            if BINARY:\n",
    "                return img, 1\n",
    "            diagnosis = label.iloc[0]['Diagnosis']\n",
    "            if diagnosis == 295.1 or diagnosis == 295.2 or diagnosis == 295.3 or diagnosis == 295.6 or diagnosis == 295.9:\n",
    "                return img, 1\n",
    "            else:\n",
    "                return img, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loads all datasets and ensures class ratio is equal in train, val and test\n",
    "'''\n",
    "\n",
    "# control_path_0 = '/data/working/oscar/data/COBRE/groups/control'\n",
    "# paranoid_path_0 = '/data/working/oscar/data/COBRE/groups/paranoid'\n",
    "# rest_path_0 = '/data/working/oscar/data/COBRE/groups/rest'\n",
    "\n",
    "control_path_1 = '/data/working/oscar/data/ERC2-I/groups/control'\n",
    "paranoid_path_1 = '/data/working/oscar/data/ERC2-I/groups/paranoid'\n",
    "rest_path_1 = '/data/working/oscar/data/ERC2-I/groups/rest'\n",
    "\n",
    "control_path_2 = '/data/working/oscar/data/ERC2-II/groups/control'\n",
    "paranoid_path_2 = '/data/working/oscar/data/ERC2-II/groups/paranoid'\n",
    "rest_path_2 = '/data/working/oscar/data/ERC2-II/groups/rest'\n",
    "\n",
    "# control_data_0 = COBRE_dataset_subj(control_path_0)\n",
    "# paranoid_data_0 = COBRE_dataset_subj(paranoid_path_0)\n",
    "# rest_data_0 = COBRE_dataset_subj(rest_path_0)\n",
    "\n",
    "control_data_1 = ERC_I_dataset_subj(control_path_1)\n",
    "paranoid_data_1 = ERC_I_dataset_subj(paranoid_path_1)\n",
    "rest_data_1 = ERC_I_dataset_subj(rest_path_1)\n",
    "\n",
    "control_data_2 = ERC_II_dataset_subj(control_path_2)\n",
    "paranoid_data_2 = ERC_II_dataset_subj(paranoid_path_2)\n",
    "rest_data_2 = ERC_II_dataset_subj(rest_path_2)\n",
    "\n",
    "# control_data = ConcatDataset([control_data_0, control_data_1, control_data_2])\n",
    "# paranoid_data = ConcatDataset([paranoid_data_0, paranoid_data_1, paranoid_data_2])\n",
    "# rest_data = ConcatDataset([rest_data_0, rest_data_1, rest_data_2])\n",
    "\n",
    "control_data = ConcatDataset([control_data_1, control_data_2])\n",
    "paranoid_data = ConcatDataset([paranoid_data_1, paranoid_data_2])\n",
    "rest_data = ConcatDataset([rest_data_1, rest_data_2])\n",
    "\n",
    "seed = torch.manual_seed(1337)\n",
    "\n",
    "train_control, test_val_control = random_split(control_data, [1-SPLIT, SPLIT], seed)\n",
    "train_paranoid, test_val_paranoid = random_split(paranoid_data, [1-SPLIT, SPLIT], seed)\n",
    "train_rest, test_val_rest = random_split(rest_data, [1-SPLIT, SPLIT], seed)\n",
    "\n",
    "val_control, test_control = random_split(test_val_control, [0.55, 0.45], seed)\n",
    "val_paranoid, test_paranoid = random_split(test_val_paranoid, [0.55, 0.45], seed)\n",
    "val_rest, test_rest = random_split(test_val_rest, [0.55, 0.45], seed)\n",
    "\n",
    "train_data = ConcatDataset([train_control, train_paranoid, train_rest])\n",
    "val_data = ConcatDataset([val_control, val_paranoid, val_rest])\n",
    "test_data = ConcatDataset([test_control, test_paranoid, test_rest])\n",
    "\n",
    "\n",
    "print('Train')\n",
    "print(len(train_control), '+', len(train_paranoid), '+', len(train_rest), '=', len(train_data))\n",
    "print()\n",
    "print('Val')\n",
    "print(len(val_control), '+', len(val_paranoid), '+', len(val_rest), '=', len(val_data))\n",
    "print()\n",
    "print('Test')\n",
    "print(len(test_control), '+', len(test_paranoid), '+', len(test_rest), '=', len(test_data))\n",
    "print()\n",
    "print('Total')\n",
    "print(len(train_control)+len(val_control)+(len(test_control)), '+', len(train_paranoid)+len(val_paranoid)+(len(test_paranoid)), '+', len(train_rest)+len(val_rest)+(len(test_rest)), '=', len(train_data)+len(val_data)+len(test_data))\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, generator=seed, num_workers=0)\n",
    "valloader = DataLoader(val_data, batch_size=1, shuffle=True, generator=seed, num_workers=0)\n",
    "# testloader = DataLoader(test_data, batch_size=1, shuffle=True, generator=seed, num_workers=0)\n",
    "\n",
    "# calculate class weights for the loss fuction\n",
    "if BINARY:\n",
    "    class_weights = [len(train_data)/(NUM_CLASSES*len(train_control)), len(train_data)/(NUM_CLASSES*(len(train_paranoid)+len(train_rest)))]\n",
    "else:\n",
    "    class_weights = [len(train_data)/(NUM_CLASSES*len(train_control)), len(train_data)/(NUM_CLASSES*len(train_paranoid)), len(train_data)/(NUM_CLASSES*len(train_rest))]\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dim_net(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate, layer_size):\n",
    "        super(dim_net, self).__init__()\n",
    "        # needs params num_classes, dropout rate and layer size\n",
    "\n",
    "        self.conv1 = (nn.Conv3d(150, layer_size, kernel_size=(3, 3, 3), padding=1))\n",
    "        self.conv1_2 = (nn.Conv3d(layer_size, 1, kernel_size=(3, 3, 3), padding=1))\n",
    "        self.pool1 = (nn.MaxPool3d(kernel_size=(2,2,2), stride=2))\n",
    "        self.dropout = (nn.Dropout(dropout_rate))\n",
    "        self.conv2 = (nn.Conv2d(47, 1, kernel_size=(3, 3), padding=1))\n",
    "        self.pool2 = (nn.MaxPool2d(kernel_size=(2,2), stride=2))\n",
    "        self.fc1 = (nn.Linear(19*19, 4*4))\n",
    "        self.fc2 = (nn.Linear(4*4, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool1(x)\n",
    "        x = torch.squeeze(x, dim=1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = dim_net(NUM_CLASSES, DROPOUT_RATE, LAYER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train function\n",
    "best_params, losses_val, losses_train, accuracies_val, accuracies_train, balanced_accuracies, balanced_accuracies_train, best_epoch = train_model(net, device, trainloader, valloader, N_EPOCHS, LEARNING_RATE, DROPOUT_RATE, LAYER_SIZE, MODE, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = dim_net(NUM_CLASSES, DROPOUT_RATE, LAYER_SIZE)\n",
    "best_model.load_state_dict(best_params)\n",
    "\n",
    "# filename = _\n",
    "# best_model = torch.load(filename, map_location=torch.device('cpu'))\n",
    "\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Val')\n",
    "y_true, y_pred = validate(best_model, device, valloader, class_weights)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train')\n",
    "y_true_train, y_pred_train = validate(best_model, device, trainloader, class_weights)\n",
    "print(classification_report(y_true_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Test')\n",
    "# y_true_test, y_pred_test = validate(best_model, device, testloader, class_weights)\n",
    "# print(classification_report(y_true_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Val')\n",
    "print('True:', y_true)\n",
    "unique, count = np.unique(y_true, return_counts=True)\n",
    "print(unique, count)\n",
    "print('Pred:', y_pred)\n",
    "unique, count = np.unique(y_pred, return_counts=True)\n",
    "print(unique, count)\n",
    "unique, count = np.unique([t==y for t,y in zip(y_true, y_pred)], return_counts=True)\n",
    "print(unique, count)\n",
    "print('Number of wrong predictions:', [count[0] if len(count) != 1 else 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train')\n",
    "print('True:', y_true_train)\n",
    "unique, count = np.unique(y_true_train, return_counts=True)\n",
    "print(unique, count)\n",
    "print('Pred:', y_pred_train)\n",
    "unique, count = np.unique(y_pred_train, return_counts=True)\n",
    "print(unique, count)\n",
    "unique, count = np.unique([t==y for t,y in zip(y_true_train, y_pred_train)], return_counts=True)\n",
    "print(unique, count)\n",
    "print('Number of wrong predictions:', [count[0] if len(count) != 1 else 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Test')\n",
    "# print('True:', y_true_test)\n",
    "# unique, count = np.unique(y_true_test, return_counts=True)\n",
    "# print(unique, count)\n",
    "# print('Pred:', y_pred_test)\n",
    "# unique, count = np.unique(y_pred_test, return_counts=True)\n",
    "# print(unique, count)\n",
    "# unique, count = np.unique([t==y for t,y in zip(y_true_test, y_pred_test)], return_counts=True)\n",
    "# print(unique, count)\n",
    "# print('Number of wrong predictions:', [count[0] if len(count) != 1 else 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = datetime.now().strftime('%d%m')\n",
    "# # date = str(1503) # March 15th\n",
    "# data_dir = '/model_acc&loss/' + date + '_' + str(LEARNING_RATE) + '_' + str(DROPOUT_RATE) + '_' +  str(LAYER_SIZE)\n",
    "\n",
    "# validation_losses = torch.load(data_dir + '/losses_val')\n",
    "# train_losses = torch.load(data_dir + '/losses_train')\n",
    "# validation_accuracies = torch.load(data_dir + '/acc_val')\n",
    "# train_accuracies = torch.load(data_dir + '/acc_train')\n",
    "# validation_bal_acc = torch.load(data_dir + '/bal_acc_val')\n",
    "# train_bal_acc = torch.load(data_dir + '/bal_acc_train')\n",
    "# best_epoch = np.argmax(validation_bal_acc) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xvalues = np.linspace(1, N_EPOCHS, len(validation_losses))\n",
    "# yvalues01 = validation_losses\n",
    "# yvalues02 = train_losses\n",
    "# name = 'Dropout=' + str(DROPOUT_RATE)\n",
    "\n",
    "# plt.plot(xvalues, yvalues01, label=name + \" val loss\", color='r')\n",
    "# plt.plot(xvalues, yvalues02, label=name + \" train loss\", color='b', linestyle=\"dashed\")\n",
    "# max_value = 4.7 #np.max([np.max(validation_losses), np.max(train_losses)])\n",
    "# min_value = np.min([np.min(validation_losses), np.min(train_losses)])\n",
    "# plt.plot([best_epoch, best_epoch], [min_value,max_value], alpha=0.5, linewidth=3, label='Chosen model')\n",
    "\n",
    "# plt.title('Validation and training loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.xticks((np.array([0.1,1,2,3,4,5,6,7,8,9,10])/10)*N_EPOCHS)\n",
    "# plt.ylim(0,5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xvalues = np.linspace(1, N_EPOCHS, len(validation_accuracies))\n",
    "# yvalues01 = validation_accuracies\n",
    "# yvalues02 = train_accuracies\n",
    "# name = 'Dropout=' + str(DROPOUT_RATE)\n",
    "\n",
    "# plt.plot(xvalues, yvalues01, label=name + \" val acc\", color='r')\n",
    "# plt.plot(xvalues, yvalues02, label=name + \" train acc\", color='b', linestyle=\"dashed\")\n",
    "# plt.plot([best_epoch, best_epoch], [0,1], alpha=0.5, linewidth=3, label='Chosen model')\n",
    "\n",
    "# plt.title('Validation and training accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.xticks((np.array([1,2,3,4,5,6,7,8,9,10])/10)*N_EPOCHS)\n",
    "# plt.yticks(np.array([0,1,2,3,4,5,6,7,8,9,10])/10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xvalues = np.linspace(1, N_EPOCHS, len(validation_bal_acc))\n",
    "# yvalues01 = validation_bal_acc\n",
    "# yvalues02 = train_bal_acc\n",
    "# name = 'Dropout=' + str(DROPOUT_RATE)\n",
    "\n",
    "# plt.plot(xvalues, yvalues01, label=name + \" val bal acc\", color='r')\n",
    "# plt.plot(xvalues, yvalues02, label=name + \" train bal acc\", color='b', linestyle=\"dashed\")\n",
    "# plt.plot([best_epoch, best_epoch], [0,1], alpha=0.5, linewidth=3, label='Chosen model')\n",
    "\n",
    "# plt.title('Validation and training balanced accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Balanced accuracy')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.xticks((np.array([1,2,3,4,5,6,7,8,9,10])/10)*N_EPOCHS)\n",
    "# plt.yticks(np.array([0,1,2,3,4,5,6,7,8,9,10])/10)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
